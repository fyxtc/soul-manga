import scrapy
import sys
import logging
import re
from manga.items import MangaItem
from manga.items import SqliteItem
import sqlite3
from hanziconv import HanziConv
import os

# æŠ“å–çš„ç±»å‹: æ•´ä¸ªç½‘ç«™ï¼Œå½“ä¸ªç±»å‹é¡µé¢(æ¯”å¦‚å†’é™©)ï¼Œå•ä¸ªæ¼«ç”»ã€‚å¯è§å‚ç…§å¯¹åº”çš„urls
# æ­¤æ—¶çš„allæœ‰é—®é¢˜ï¼Œå› ä¸ºhttp://www.cartoonmad.com/comic99.100çš„æ•°æ®å’Œ99.01æ˜¯ä¸€æ ·çš„ï¼Œcardtonnmadçš„bug
# å¯ä»¥ä½¿ç”¨REQ_PAGEï¼Œç„¶åæ‰“å¼€page_urlsçš„æ‰€æœ‰æ³¨é‡Šè¾¾åˆ°åŒæ ·çš„æ•ˆæœ
REQ_DEFAULT, REQ_ALL, REQ_PAGE, REQ_SINGLE = -1, 0, 1, 2
# é»˜è®¤ä¸æ›´æ–°ï¼Œä¸”ä¸æŠ“å–ä»»ä½•é¡µé¢ï¼Œä½¿ç”¨é¡¹ç›®è‡ªå¸¦çš„soul_manga.db
IS_UPDATE = False 
REQ_TYPE = REQ_DEFAULT
# REQ_TYPE = REQ_ALL

class SoulMangaSpider(scrapy.Spider):
    name = "soul_manga"
    xpath = {
        "single_urls": ["http://www.cartoonmad.com/comic/1152.html"],
        # "index_urls": ["http://www.cartoonmad.com/comic99.html"],
        "index_urls": [
            "http://www.cartoonmad.com/comic01.html",
            "http://www.cartoonmad.com/comic02.html",
            "http://www.cartoonmad.com/comic03.html",
            "http://www.cartoonmad.com/comic04.html",
            "http://www.cartoonmad.com/comic07.html",
            "http://www.cartoonmad.com/comic08.html",
            "http://www.cartoonmad.com/comic09.html",
            "http://www.cartoonmad.com/comic10.html",
            "http://www.cartoonmad.com/comic13.html",
            "http://www.cartoonmad.com/comic14.html",
            "http://www.cartoonmad.com/comic16.html",
            "http://www.cartoonmad.com/comic17.html",
            "http://www.cartoonmad.com/comic18.html",
            "http://www.cartoonmad.com/comic21.html",
            "http://www.cartoonmad.com/comic22.html",
        ],  

        "update_urls": ["http://www.cartoonmad.com/newcm.html"],
        "next_page": "//a[contains(., 'ä¸‹ä¸€é ')]/@href",
        # æ‰€æœ‰åˆ—åˆ«çš„urls
        "page_urls": [
            "http://www.cartoonmad.com/comic01.html",
            # "http://www.cartoonmad.com/comic02.html",
            # "http://www.cartoonmad.com/comic03.html",
            # "http://www.cartoonmad.com/comic04.html",
            # "http://www.cartoonmad.com/comic07.html",
            # "http://www.cartoonmad.com/comic08.html",
            # "http://www.cartoonmad.com/comic09.html",
            # "http://www.cartoonmad.com/comic10.html",
            # "http://www.cartoonmad.com/comic13.html",
            # "http://www.cartoonmad.com/comic14.html",
            # "http://www.cartoonmad.com/comic16.html",
            # "http://www.cartoonmad.com/comic17.html",
            # "http://www.cartoonmad.com/comic18.html",
            # "http://www.cartoonmad.com/comic21.html",
            # "http://www.cartoonmad.com/comic22.html",
        ],  
        "urls": ["http://www.cartoonmad.com/comic/1090.html"],
        "chapter": "//a[contains(., 'è©±') and contains(., 'ç¬¬')]/@href",  # é»˜è®¤ä¸‹è½½è¯
        "vol": "//a[contains(., 'å·')]/@href",  # é»˜è®¤ä¸‹è½½è¯
        "image_page": "//option[contains(., 'é ')]/@value", # éå†è¿™ä¸€è¯çš„æ‰€æœ‰imgé¡µçš„è¶…é“¾æ¥  
        "image": "//img[contains(@src, 'cartoonmad.com')]/@src", #è¿™ä¸€è¯çš„å›¾ç‰‡

        # ä¸çŸ¥é“ä¸ºå•¥ï¼Œchromeé‡Œé¢å¯ä»¥çš„ï¼Œscrapyä¸€ç¢°åˆ°tbodyå°±è·ªã€‚ã€‚åŸå› å¦‚ä¸‹ã€‚ã€‚åŸæ¥tbodyæ˜¯æµè§ˆå™¨åŠ çš„...é‚£å°±ç®€å•äº†ï¼Œæˆ‘å»äº†å°±è¡Œäº†ã€‚ã€‚
        # Firefox, in particular, is known for adding <tbody> elements to tables. Scrapy, on the other hand, does not modify the original page HTML, 
        # so you wonâ€™t be able to extract any data if you use <tbody> in your XPath expressions.

        "mid":"/html/body/table/tr[1]/td[2]/table/tr[3]/td[2]/a[3]/@href",
        "name":"/html/body/table/tr[1]/td[2]/table/tr[3]/td[2]/a[3]/text()",
        "author":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[5]/td/text()",
        # todo: è¿˜æœ‰åŠ¨æ€å°é¢å§æ§½å…¶å®è¿™ä¸ªä¸ç”¨çˆ¬ï¼Œé€šè¿‡midå°±èƒ½çŸ¥é“äº†ï¼Œhttp://img.cartoonmad.com/ctimg/1490.jpgï¼Œ http://img.cartoonmad.com/ctimg/1490.jpg,å¥½åƒå°±è¿™ä¸¤ä¸ªåœ°æ–¹...ä¸Šä¸‹åˆæ¢çš„ï¼Ÿinteresting
        "cover_image":"//div[@class='cover']/../img/@src",
        "cover_update_info":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[7]/td/font/text()",
        "category":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[3]/td/a[1]/text()",
        # todo: åµŒå¥—çš„<p>æ²¡æœ‰å®ç°ï¼Œæ¯”å¦‚æ£‹é­‚...
        # "summary":"//legend[contains(., 'ç°¡ä»‹')]/../table/tr/td/text()",
        # stringæŠ“å–åµŒå¥—æ–‡æœ¬
        "summary":"string(//legend[contains(., 'ç°¡ä»‹')]/../table/tr/td)",

        "last_update_date":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[1]/td[2]/b/font/text()",
        "status":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[7]/td/img[2]/@src", #chap9.gif
        "pop":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[11]/td/text()",
        "tags":"//td[contains(., 'æ¼«ç•«æ¨™ç±¤')]/./a/text()",
        "chapters":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[7]/td/font/text()", 
        "chapter_images":"",
        # "vol_or_ch":"",

        "all_chapters": "//a[contains(., 'è©±') and contains(., 'ç¬¬')]/text()",
        "all_chapters_pages": "//a[contains(., 'è©±') and contains(., 'ç¬¬')]/../font/text()",
        "all_vols": "//a[contains(., 'å·') and contains(., 'ç¬¬')]/text()",
        "all_vols_pages": "//a[contains(., 'å·') and contains(., 'ç¬¬')]/../font/text()",
        # "image_base_url": "/html/body/table/tr[5]/td/a/img/@src"
        "image_base_url": "//img[contains(@src, 'cartoonmad.com')]/@src", #è¿™ä¸€è¯çš„å›¾ç‰‡
    }
    sql_item = {}


    # def __init__(self, is_update, *args, **kwargs):
    #     super(SoulMangaSpider, self).__init__(*args, **kwargs)

    def get_chapter(self, ch):
        index1 = ch.find("ç¬¬")
        index2 = ch.find("è©±")
        return int(str.strip(ch[index1+1: index2]))

    def get_sql_item(self, response):
        # å¼‚æ­¥ä»£ç ï¼Œä¸èƒ½é€šè¿‡selfè·å–ï¼Œè¦ç›´æ¥ä¼ é€’ä¸‹å»ï¼Œé€šè¿‡meta
        sql_item = {}
        sql_item["mid"] = response.xpath(self.xpath.get("mid")).extract_first()
        sql_item["name"] = response.xpath(self.xpath.get("name")).extract_first()
        sql_item["author"] = response.xpath(self.xpath.get("author")).extract_first()
        sql_item["cover_image"] = response.xpath(self.xpath.get("cover_image")).extract_first()
        sql_item["cover_update_info"] = response.xpath(self.xpath.get("cover_update_info")).extract_first()
        sql_item["category"] = response.xpath(self.xpath.get("category")).extract_first()
        sql_item["summary"] = response.xpath(self.xpath.get("summary")).extract_first()
        sql_item["last_update_date"] = response.xpath(self.xpath.get("last_update_date")).extract()[1]
        sql_item["status"] = response.xpath(self.xpath.get("status")).extract_first()
        sql_item["pop"] = response.xpath(self.xpath.get("pop")).extract_first()
        sql_item["tags"] = response.xpath(self.xpath.get("tags")).extract()

        chapters = response.xpath(self.xpath.get("all_chapters")).extract()
        sql_item["all_chapters_len"] = len(chapters)
        sql_item["all_chapters_pages"] = response.xpath(self.xpath.get("all_chapters_pages")).extract()
        sql_item["chapter_start_index"] = 1 if len(chapters) == 0 else self.get_chapter(chapters[0])
        sql_item["last_update_chapter"] = 0 if len(chapters) == 0 else self.get_chapter(chapters[-1])
        sql_item["last_update_vol_or_ch"] = 1 if len(chapters) == 0 else 0

        vols = response.xpath(self.xpath.get("all_vols")).extract()
        sql_item["all_vols_len"] = len(vols)
        sql_item["all_vols_pages"] = response.xpath(self.xpath.get("all_vols_pages")).extract()

        for k, v in sql_item.items():
            if isinstance(v, str) and k != "last_update_date":
                v = re.sub(r"\s+", "", v, flags=re.UNICODE)
            if isinstance(v, str):
                v = HanziConv.toSimplified(v)

            if k == "mid":
                v = int(v[v.rfind("/")+1:v.rfind(".")])
            elif k in ["author", "pop"]:
                v = v[v.find("ï¼š")+1:]
            elif k == "category":
                v = self.get_category(v) # remove ç³»åˆ—
            elif k == 'last_update_date':
                temp = str.strip(v)
                v = temp[temp.find(" ")+1:]
            elif k == "status":
                v = "å·²å®Œç»“" if v.find("chap9.gif") != -1 else "è¿è½½ä¸­"
            elif k == "all_chapters_pages" or k == "all_vols_pages":
                temp = [re.findall(r"\d+", x)[0] for x in v]
                v = ','.join(temp)
            elif k == "tags":
                v = ','.join(v)
            elif k == "summary":
                v = str.strip(v)
            sql_item[k] = v

        # for k, v in sql_item.items():
        #     print(k + ": " + str(v))
        return sql_item

    def get_category(self, ori):
        category_map = [
            'æ ¼æ–—',
            'é­”æ³•',
            'ä¾¦æ¢',
            'ç«æŠ€',
            'ææ€–',
            'æˆ˜å›½',
            'é­”å¹»',
            'å†’é™©',
            'æ ¡å›­',
            'æç¬‘',
            'å°‘å¥³',
            'å°‘ç”·',
            'ç§‘å¹»',
            'æ¸¯äº§',
            'å…¶ä»–' 
        ]
        cat = ori[:2]
        assert (cat in category_map), cat+" wtf?"
        return category_map.index(cat)

    def start_requests(self):
        self.sql = None
        self.values = []
        self.sqlite_file = self.settings.get("SQLITE_FILE")
        self.sqlite_table = self.settings.get("SQLITE_TABLE")

        # self.log("fuck " + self.sqlite_file + ", " + self.sqlite_table)
        self.conn = sqlite3.connect(self.sqlite_file)
        self.cur = self.conn.cursor()
        exist = self.cur.execute("pragma table_info('soul_manga')").fetchone()
        logging.info("exist table soul_mange? ------- " + str(exist))
        if not exist:
            # print(os.system("pwd"))
            os.system('sqlite3 ../server/soul_manga.db ".read ../server/soul_manga.sql"')

        logging.info("IS_UPDATE >>>>>>>>>>>>>>>>>>> " + str(IS_UPDATE))

        # æœ¬åœ°è·‘å§ã€‚ã€‚ã€‚ã€‚vps crontabå„ç§å‘½ä»¤æ‰¾ä¸åˆ°ï¼Œå¥½çƒ¦= =
        # æœ¬åœ°çš„è¯éœ€è¦cronçˆ¬å–ç„¶åæ¥ç€è¿è¡Œfab deployï¼Œå¯ä»¥å†™ä¸ªfab updateï¼Œç„¶åè®©cronè°ƒå–fab update => fab deployj
        if IS_UPDATE:
            # æŠ“å–æ›´æ–°  æ›´æ–°å…¶å®æœ€å¥½æ˜¯å¦å¤–æ”¾åœ¨åˆ«çš„è„šæœ¬é‡Œï¼Œç„¶åå®šæ—¶ä»»åŠ¡å»è°ƒç”¨æ‰æ˜¯æœ€å¥½çš„ï¼Œå…ˆæ‰‹åŠ¨æ³¨é‡Šæ‰“å¼€å§ã€‚æˆ–è€…ä¼ å‘½ä»¤è¡Œå‚æ•°ä¹Ÿå¯ä»¥å“¦ï¼Œæœºæ™ºå¦‚æˆ‘
            logging.info("start update crawl >>>>>>>>>>>> ")
            # æ³¨æ„è¿™é‡ŒåªæŠ“å–æ¯æ—¥çš„æ›´æ–°é¡µï¼Œæ‰€ä»¥å¦‚æœéš”äº†ä¸€å¤©æ²¡æ›´æ–°ï¼Œç„¶åæƒ³æ›´æ–°å…¨éƒ¨çš„è¯ï¼Œè¿˜æ˜¯è¿˜æ˜¯å¾—èµ°å…¨é‡å“¦
            urls = self.xpath.get("update_urls")
            for url in urls:
                yield scrapy.Request(url=url, callback=self.parse_update)
        else:
            # è‡ªå·±æŒ‰éœ€è¦æ‰“å¼€ï¼Œå¹¶ä¸”æ ¹æ®éœ€è¦è°ƒæ•´page_urlså’Œsingle_urls

            if REQ_TYPE == REQ_ALL:
                # è·å–å…¨éƒ¨æ¼«ç”»
                urls = self.xpath.get("index_urls")
                for url in urls:
                    yield scrapy.Request(url=url, callback=self.parse_index)
            elif REQ_TYPE == REQ_PAGE:
                # è·å–å…¨é¡µæ¼«ç”»
                urls = self.xpath.get("page_urls")
                # æ˜¯å¦åªè·å–å½“å‰pageï¼Œè¿˜æ˜¯è¦è·å–è¿™ä¸ªç±»åˆ«ä¸‹çš„æ‰€æœ‰ä¸‹ä¸€é¡µ
                only_cur_page = True
                for url in urls:
                    yield scrapy.Request(url=url, callback=self.parse_page if only_cur_page else self.parse_index)
            elif REQ_TYPE == REQ_SINGLE:
                # è·å–å•ä¸ªæ¼«ç”»
                urls = self.xpath.get("single_urls")
                for url in urls:
                    yield scrapy.Request(url=url, callback=self.parse)
            else:
                logging.error("\n\n <<<<<<<<<<<<<<<<< WHAT YOU WANT MAN, PLEASE SPECIFY ONE CRAWL FORMATï¼š ALL OR PAGE OR SINGLE !!!!!!!!!!!! >>>>>>>>>>>>> \n")


    # åœ¨vpsä¸Šç”¨cronèµ·äº†å®šæ—¶ä»»åŠ¡å»çˆ¬å–æ›´æ–°äº†ï¼Œä¹Ÿå°±æ˜¯è¯´æœ¬åœ°å¦‚æœæ”¹äº†dbä¹‹åï¼Œé‚£å¿…é¡»æ˜¯å®Œå…¨é‡æ–°æŠ“å–ï¼Œæˆ–è€…è®¾ç½®çˆ¬å–å‰å‡ ä¸ªé¡µé¢æ‰æ˜¯æœ€æ–°çš„
    # çˆ¬è™«è„šæœ¬æˆ‘å°±æ²¡æœ‰æ”¾åˆ°fab deployé‡Œé¢å»äº†ï¼Œç›´æ¥ä¸Šä¼ è¿™ä¸ªä½œä¸ºæ›´æ–°ç‰ˆæœ¬å°±è¡Œäº†
    def parse_update(self, response):
        # æŠ“å–æ›´æ–°å’ŒæŠ“å–æ™®é€šçš„é¡µé¢å¹¶æ²¡æœ‰å¾ˆå¤§åŒºåˆ«ï¼Œåªæ˜¯è¦æ³¨æ„å†™å…¥æ•°æ®åº“çš„æ—¶å€™ä¸èƒ½ä»…ä»…é€šè¿‡midåˆ¤æ–­ï¼Œè¿˜è¦æœ‰update_time
        mangas = re.findall(r"comic/\d{4}.html", str(response.body))#[:20]
        if response.url.find("/comic/") != -1:
            mangas = [x[6:] for x in mangas]
        urls = {response.urljoin(x) for x in mangas}
        # logging.info(urls)

        # # è¿™æ ·å°±æŠŠå½“å‰é¡µ(page_urls)åŒ…å«çš„æ‰€æœ‰æ¼«ç”»éƒ½çˆ¬äº†ğŸ˜¯
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse, meta={"is_update": True})

    def parse_index(self, response):
        next_url = response.xpath(self.xpath.get("next_page")).extract_first()
        # print("parse_index next_url " + str(next_url))
        next_url = response.urljoin(next_url) 
        # logging.info("next url " + next_url)
        # # for url in urls:
        # #     logging.info("fuck next ")
        # #     yield scrapy.Request(url=url, callback=self.parse_page)
        return self.parse_page(response, next_url)


    def parse_page(self, response, next_url=None):
        # print("parse_page next_url " + str(next_url))
        mangas = re.findall(r"comic/\d{4}.html", str(response.body))#[:20]
        if response.url.find("/comic/") != -1:
            mangas = [x[6:] for x in mangas]
        # self.log(mangas)
        # é›†åˆæ¨å¯¼ä½¿ç”¨{}
        urls = {response.urljoin(x) for x in mangas}
        # self.log(urls)

        # # è¿™æ ·å°±æŠŠå½“å‰é¡µ(page_urls)åŒ…å«çš„æ‰€æœ‰æ¼«ç”»éƒ½çˆ¬äº†ğŸ˜¯
        # print(urls)
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse, meta={"next_url": next_url})


    def parse(self, response):
        # å…¶å®è¿™é‡Œæœ¬æ¥æ¯ä¸ªæ¼«ç”»çš„urlä¹Ÿå°±èµ°ä¸€æ¬¡å§ã€‚ã€‚ã€‚ç®€ç›´å®Œç¾
        item = self.get_sql_item(response)
        # logging.info(item)
        
        mid = item.get("mid")
        last_update_date = item.get("last_update_date")
        if not self.is_need_insert_or_update(mid, last_update_date):
            # todo è¿™é‡Œæœ‰bugï¼Œä¸èƒ½midå­˜åœ¨å°±è·³è¿‡å•Šã€‚ã€‚ã€‚è¿™æ ·èµ°ä¸äº†next_urlçš„è¯·æ±‚äº†ï¼Œæˆ‘å…ˆå…¨éƒ¨æ¸…é™¤äº†æ¥è¿‡å§ã€‚ã€‚ã€‚è¿™æ ·æ²¡é—®é¢˜ã€‚ã€‚ã€‚ä¸ã€‚å…ˆçˆ¬åˆ°spideræ–‡ä»¶å¤¹ä¸‹çš„dbå§ï¼Œæ”¹setting
            # ä½†æ˜¯å¢é‡æ›´æ–°è¿™é‡Œæ˜¯ç»•ä¸å¼€çš„ï¼Œå¿…é¡»æƒ³åŠæ³•ï¼Œåˆ¤æ–­æœ€åæ›´æ–°æ—¥æœŸæ˜¯å¦ä¸€æ ·ï¼Œå¦‚æœä¸ä¸€æ ·å°±update chapterå­—æ®µ
            # ç„¶åæ¯å¤©çš„è®¡åˆ’åº”è¯¥æ˜¯0ç‚¹çˆ¬â€œæœ€æ–°ä¸Šæ¶â€é¡µé¢çš„å¤´å‡ é¡µï¼Œå¤´ä¸¤é¡µåŸºæœ¬ä¸Šèƒ½ä¿è¯å½“å¤©çš„æ›´æ–°åº¦äº†ï¼Œå…¶å®åº”è¯¥ä¸€é¡µå°±è¡Œäº†ã€‚ã€‚ä¿å®ˆèµ·è§å§
            logging.info("mid {0} is exist and last_update_date is same, skip ".format(mid))
            return
        url = response.xpath(self.xpath.get("chapter")).extract_first()
        if not url:
            # è¿™é‡Œå¥½å¥‡æ€ªå•Šã€‚ã€‚ã€‚æˆ‘è¿™æ ·å†™æ˜æ˜åªèƒ½è·å–ä¸€ä¸ªï¼Œè¦ä¹ˆæ˜¯volè¦ä¹ˆæ˜¯chapter....æ€ä¹ˆæ²¡é—®é¢˜å‘¢
            url = response.xpath(self.xpath.get("vol")).extract_first()
        assert url, response.url +" is fuck" 
        first_chapter_url = response.urljoin(url)
        yield scrapy.Request(url=first_chapter_url, callback=self.parse_image_base_url, meta={"item": item, "next_url": response.meta.get("next_url")})

    # todo: å¦‚æœå½“å‰é¡µå…¨éƒ¨skipäº†çš„è¯ï¼Œè¿™é‡Œè¿›ä¸æ¥ï¼Œé‚£ä¹ˆnext_urlå°±å¤±æ•ˆäº†
    def parse_image_base_url(self, response):
        # logging.info(response)
        # logging.info(response.xpath(self.xpath.get("image_base_url")))
        url = response.xpath(self.xpath.get("image_base_url")).extract_first()
        # logging.info(response.url + ", " + str(url))
        item = response.meta.get("item")
        # logging.info(item)
        assert item != None
        assert url != None
        mid = item.get("mid")
        image_base_url = url[:url.find("/"+str(mid)+"/")]
        item["image_base_url"] = image_base_url
        # self.log(image_base_url)
        # logging.info(item)

        self.write_database(item)


        next_url = response.meta.get("next_url")
        if next_url:
            # logging.info("next url: " + next_url)
            yield scrapy.Request(url=next_url, callback=self.parse_index)

        # è§£æä¸€ä¸ªå®Œæˆäº†ä¹‹åï¼ŒåŠ å…¥next_page_urlï¼Œå¯ä»¥è¿™æ ·æ¯ä¸€ä¸ªitemè§£æå®Œéƒ½ä¼šè¯·æ±‚è¿™ä¸ªï¼Œè™½ç„¶scrapyè‡ªå·±å»é‡äº†ï¼Œä½†æ˜¯æ¯•ç«Ÿå¤šä¸€æ­¥åˆ¤æ–­ï¼Œå…ˆè¿™æ ·å†™å§

    def write_database(self, item):
        # è¿™ä¸ªå†™æ³•ç¡®å®åŠï¼Œä½†æ˜¯è¦æ³¨æ„.values()2/3è¡¨ç°å¥½åƒä¸ä¸€æ ·ï¼Œ3ä¼šæœ‰dictvalueä¹‹ç±»çš„å­—ç¬¦ä¸²ï¼Œæ‰€ä»¥å’Œkeysä¸€æ ·ç”¨joinè¿æ¥å§ï¼Œä½†æ˜¯ã€‚ã€‚ã€‚intå°±è·ªäº†æ¡è‰ï¼Œè¿™æ€ä¹ˆæ•´ï¼Œè½¬tunpleå°±å¥½äº†
        sql = 'insert or replace into {0} ({1}) values ({2})'.format(self.sqlite_table, ', '.join(item.keys()), ', '.join(['?'] * len(item.keys())))
        if not self.sql:
            self.sql = sql
        # logging.info(sql)
        logging.info("insert or replace mid " + str(item.get("mid")) + ": " + item.get("name") + " category: " + str(item.get("category")))
        values = tuple(item.values())
        # self.log(values)

        # ç©ºé—´æ¢æ—¶é—´ï¼Œå…ˆå­˜sqlæœ€åç»Ÿä¸€è°ƒç”¨ã€‚è¿˜èƒ½ä¼˜åŒ–ä¸ºä¸€æ¬¡æ’å…¥ï¼Œå› ä¸ºæ¯æ¬¡sqlçš„statéƒ½æ˜¯ä¸€æ ·çš„
        # æ²¡æœ‰åµç”¨ï¼Œå› ä¸ºè¿™ä¸æ˜¯ç“¶é¢ˆã€‚ã€‚ã€‚è¿˜æ˜¯è·å–respè€—æ—¶é—´,4000æ¡å†™å…¥çš„ä¼˜åŒ–ä¹Ÿå°±å‡ ç§’çš„æ—¶é—´
        # self.values.append(values)

        self.cur.execute(sql, values)
        self.conn.commit()

    def is_need_insert_or_update(self, mid, last_update_date):
        # midä¸ç”¨å–äº†ï¼Œå–æ—¶é—´ä¸€æ ·çš„ï¼Œå°±èƒ½çŸ¥é“åœ¨ä¸åœ¨äº† 
        sql = "select last_update_date from {0} where mid = ? ".format(self.sqlite_table)
        # é€—å·æ˜¯å¿…é¡»çš„ï¼Œä¸ç„¶ä¼šè¢«è§£ææˆæ‹¬å·ï¼Œè€Œä¸æ˜¯tunple
        cursor = self.cur.execute(sql, (mid, ))
        res = cursor.fetchone()
        # logging.info(res)
        if res == None:
            # æ²¡æœ‰çš„è¯ï¼Œè‚¯å®šè¦æ’å…¥
            # logging.info(str(mid) + " is not exist, insert it ")
            return True
        else:
            # å¦‚æœæœ‰çš„è¯ï¼Œçœ‹æ›´æ–°æ—¥æœŸæ˜¯å¦ä¸€æ ·ï¼Œæ³¨æ„ï¼Œè¿™ä¸ªæ— è®ºæ˜¯å¦æ˜¯æ›´æ–°è°ƒç”¨è¿‡æ¥çš„éƒ½éœ€è¦èµ°
            db_last_update_date = res[0]
            if last_update_date != db_last_update_date:
                logging.info("update mid " + str(mid) + " old date " + db_last_update_date + " ==> " + last_update_date)
                return True
        return False
        # æŸ¥è¯¢ä¸ç”¨commitï¼Œsaveçš„æ“ä½œæ‰éœ€è¦
        # self.conn.commit()

    def write_all_sqls(self):
        assert self.cur
        logging.info("write all sqls/manga count >>>>>>>>>>>>>>> " + str(len(self.values)))
        # print("write all sqls/manga count >>>>>>>>>>>>>>> " + str(len(self.values)))
        import time
        t1 = time.time()
        for v in self.values:
            self.cur.execute(self.sql, v)
        self.conn.commit() 
        t2 = time.time()
        print("cost time " + str(t2-t1))


    def closed(self, reason):
        if self.conn:
            # self.write_all_sqls()
            self.conn.close()















