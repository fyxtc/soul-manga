import scrapy
import sys
import logging
import re
from manga.items import MangaItem
from manga.items import SqliteItem
import sqlite3
from hanziconv import HanziConv

class SoulMangaSpider(scrapy.Spider):
    name = "soul_manga"
    xpath = {
        "index_urls": ["http://www.cartoonmad.com/comic99.html"],
        "next_page": "//a[contains(., 'ä¸‹ä¸€é ')]/@href",
        "page_urls": [
            "http://www.cartoonmad.com/comic01.html",
            # "http://www.cartoonmad.com/comic02.html",
            # "http://www.cartoonmad.com/comic03.html",
            # "http://www.cartoonmad.com/comic04.html",
            # "http://www.cartoonmad.com/comic07.html",
            # "http://www.cartoonmad.com/comic08.html",
            # "http://www.cartoonmad.com/comic09.html",
            # "http://www.cartoonmad.com/comic10.html",
            # "http://www.cartoonmad.com/comic13.html",
            # "http://www.cartoonmad.com/comic14.html",
            # "http://www.cartoonmad.com/comic16.html",
            # "http://www.cartoonmad.com/comic17.html",
            # "http://www.cartoonmad.com/comic18.html",
            # "http://www.cartoonmad.com/comic21.html",
            # "http://www.cartoonmad.com/comic22.html",
        ],  
        "urls": ["http://www.cartoonmad.com/comic/1090.html"],
        "chapter": "//a[contains(., 'è©±') and contains(., 'ç¬¬')]/@href",  # é»˜è®¤ä¸‹è½½è¯
        "vol": "//a[contains(., 'å·')]/@href",  # é»˜è®¤ä¸‹è½½è¯
        "image_page": "//option[contains(., 'é ')]/@value", # éå†è¿™ä¸€è¯çš„æ‰€æœ‰imgé¡µçš„è¶…é“¾æ¥  
        "image": "//img[contains(@src, 'cartoonmad.com')]/@src", #è¿™ä¸€è¯çš„å›¾ç‰‡

        # ä¸çŸ¥é“ä¸ºå•¥ï¼Œchromeé‡Œé¢å¯ä»¥çš„ï¼Œscrapyä¸€ç¢°åˆ°tbodyå°±è·ªã€‚ã€‚åŸå› å¦‚ä¸‹ã€‚ã€‚åŸæ¥tbodyæ˜¯æµè§ˆå™¨åŠ çš„...é‚£å°±ç®€å•äº†ï¼Œæˆ‘å»äº†å°±è¡Œäº†ã€‚ã€‚
        # Firefox, in particular, is known for adding <tbody> elements to tables. Scrapy, on the other hand, does not modify the original page HTML, 
        # so you wonâ€™t be able to extract any data if you use <tbody> in your XPath expressions.

        "mid":"/html/body/table/tr[1]/td[2]/table/tr[3]/td[2]/a[3]/@href",
        "name":"/html/body/table/tr[1]/td[2]/table/tr[3]/td[2]/a[3]/text()",
        "author":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[5]/td/text()",
        # todo: è¿˜æœ‰åŠ¨æ€å°é¢å§æ§½å…¶å®è¿™ä¸ªä¸ç”¨çˆ¬ï¼Œé€šè¿‡midå°±èƒ½çŸ¥é“äº†ï¼Œhttp://img.cartoonmad.com/ctimg/1490.jpgï¼Œ http://img.cartoonmad.com/ctimg/1490.jpg,å¥½åƒå°±è¿™ä¸¤ä¸ªåœ°æ–¹...ä¸Šä¸‹åˆæ¢çš„ï¼Ÿinteresting
        "cover_image":"//div[@class='cover']/../img/@src",
        "cover_update_info":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[7]/td/font/text()",
        "category":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[3]/td/a[1]/text()",
        # todo: åµŒå¥—çš„<p>æ²¡æœ‰å®ç°ï¼Œæ¯”å¦‚æ£‹é­‚...
        "summary":"//legend[contains(., 'ç°¡ä»‹')]/../table/tr/td/text()",

        "last_update_date":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[1]/td[2]/b/font/text()",
        "status":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[7]/td/img[2]/@src", #chap9.gif
        "pop":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[11]/td/text()",
        "tags":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[13]/td/text()",
        "chapters":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[7]/td/font/text()", 
        "chapter_images":"",
        "vol_or_ch":"", #é€šè¿‡chapter/volè®¾ç½®ï¼Œä¼˜å…ˆè¯

        "all_chapters": "//a[contains(., 'è©±')]/../a/text()",
        "all_chapters_pages": "//a[contains(., 'è©±') and contains(., 'ç¬¬')]/../font/text()",
        "all_vols": "//a[contains(., 'å·')]/../a/text()",
        "all_vols_pages": "//a[contains(., 'å·')]/../font/text()",
        "image_base_url": "/html/body/table/tr[5]/td/a/img/@src"
    }
    sql_item = {}

    def get_sql_item(self, response):
        # å¼‚æ­¥ä»£ç ï¼Œä¸èƒ½é€šè¿‡selfè·å–ï¼Œè¦ç›´æ¥ä¼ é€’ä¸‹å»ï¼Œé€šè¿‡meta
        sql_item = {}
        sql_item["mid"] = response.xpath(self.xpath.get("mid")).extract_first()
        sql_item["name"] = response.xpath(self.xpath.get("name")).extract_first()
        sql_item["author"] = response.xpath(self.xpath.get("author")).extract_first()
        sql_item["cover_image"] = response.xpath(self.xpath.get("cover_image")).extract_first()
        sql_item["cover_update_info"] = response.xpath(self.xpath.get("cover_update_info")).extract_first()
        sql_item["category"] = response.xpath(self.xpath.get("category")).extract_first()
        sql_item["summary"] = response.xpath(self.xpath.get("summary")).extract()[0]
        sql_item["last_update_date"] = response.xpath(self.xpath.get("last_update_date")).extract()[1]
        sql_item["status"] = response.xpath(self.xpath.get("status")).extract_first()
        sql_item["pop"] = response.xpath(self.xpath.get("pop")).extract_first()
        sql_item["tags"] = response.xpath(self.xpath.get("tags")).extract_first()

        chapters_len = len(response.xpath(self.xpath.get("all_chapters")).extract())
        if(chapters_len != 0):
            sql_item["all_chapters_len"] = chapters_len
            sql_item["all_chapters_pages"] = response.xpath(self.xpath.get("all_chapters_pages")).extract()
            sql_item["vol_or_ch"] = 0
        else:
            sql_item["all_chapters_len"] = len(response.xpath(self.xpath.get("all_vols")).extract())
            sql_item["all_chapters_pages"] = response.xpath(self.xpath.get("all_vols_pages")).extract()
            sql_item["vol_or_ch"] = 1

        for k, v in sql_item.items():
            if isinstance(v, str) and k != "last_update_date":
                v = re.sub(r"\s+", "", v, flags=re.UNICODE)
            if isinstance(v, str):
                v = HanziConv.toSimplified(v)

            if k == "mid":
                v = int(v[v.rfind("/")+1:v.rfind(".")])
            elif k in ["author", "tags", "pop"]:
                v = v[v.find("ï¼š")+1:]
            elif k == "category":
                v = self.get_category(v) # remove ç³»åˆ—
            elif k == 'last_update_date':
                temp = str.strip(v)
                v = temp[temp.find(" ")+1:]
            elif k == "status":
                v = "å·²å®Œç»“" if v.find("chap9.gif") != -1 else "è¿è½½ä¸­"
            elif k == "all_chapters_pages":
                temp = [re.findall(r"\d+", x)[0] for x in v]
                v = ','.join(temp)
            sql_item[k] = v

        # for k, v in sql_item.items():
        #     print(k + ": " + str(v))
        return sql_item

    def get_category(self, ori):
        category_map = [
            'æ ¼æ–—',
            'é­”æ³•',
            'ä¾¦æ¢',
            'ç«æŠ€',
            'ææ€–',
            'æˆ˜å›½',
            'é­”å¹»',
            'å†’é™©',
            'æ ¡å›­',
            'æç¬‘',
            'å°‘å¥³',
            'å°‘ç”·',
            'ç§‘å¹»',
            'æ¸¯äº§',
            'å…¶ä»–' 
        ]
        cat = ori[:2]
        assert (cat in category_map), cat+" wtf?"
        return category_map.index(cat)

    def start_requests(self):
        self.sqlite_file = self.settings.get("SQLITE_FILE")
        self.sqlite_table = self.settings.get("SQLITE_TABLE")
        # self.log("fuck " + self.sqlite_file + ", " + self.sqlite_table)
        self.conn = sqlite3.connect(self.sqlite_file)
        self.cur = self.conn.cursor()

        # è·å–å…¨éƒ¨æ¼«ç”»
        urls = self.xpath.get("index_urls")
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse_index)

        # è·å–å…¨é¡µæ¼«ç”»
        # urls = self.xpath.get("page_urls")
        # for url in urls:
        #     yield scrapy.Request(url=url, callback=self.parse_page)

        # è·å–å•ä¸ªæ¼«ç”»
        # urls = self.xpath.get("urls")
        # for url in urls:
        #     yield scrapy.Request(url=url, callback=self.parse)

    def parse_index(self, response):
        next_url = response.xpath(self.xpath.get("next_page")).extract_first()
        next_url = response.urljoin(next_url) 
        # logging.info("next url " + next_url)
        # # for url in urls:
        # #     logging.info("fuck next ")
        # #     yield scrapy.Request(url=url, callback=self.parse_page)
        return self.parse_page(response, next_url)


    def parse_page(self, response, next_url):
        mangas = re.findall(r"comic/\d{4}.html", str(response.body))#[:20]
        if response.url.find("/comic/") != -1:
            mangas = [x[6:] for x in mangas]
        # self.log(mangas)
        # é›†åˆæ¨å¯¼ä½¿ç”¨{}
        urls = {response.urljoin(x) for x in mangas}
        # self.log(urls)

        # # è¿™æ ·å°±æŠŠå½“å‰é¡µ(page_urls)åŒ…å«çš„æ‰€æœ‰æ¼«ç”»éƒ½çˆ¬äº†ğŸ˜¯
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse, meta={"next_url": next_url})


    def parse(self, response):
        # å…¶å®è¿™é‡Œæœ¬æ¥æ¯ä¸ªæ¼«ç”»çš„urlä¹Ÿå°±èµ°ä¸€æ¬¡å§ã€‚ã€‚ã€‚ç®€ç›´å®Œç¾
        item = self.get_sql_item(response)
        mid = item.get("mid")
        if self.is_mid_exist(mid):
            # todo è¿™é‡Œæœ‰bugï¼Œä¸èƒ½midå­˜åœ¨å°±è·³è¿‡å•Šã€‚ã€‚ã€‚è¿™æ ·èµ°next_urlçš„è¯·æ±‚äº†ï¼Œæˆ‘å…ˆå…¨éƒ¨æ¸…é™¤äº†æ¥è¿‡å§ã€‚ã€‚ã€‚è¿™æ ·æ²¡é—®é¢˜ã€‚ã€‚ã€‚ä¸ã€‚å…ˆçˆ¬åˆ°spideræ–‡ä»¶å¤¹ä¸‹çš„dbå§ï¼Œæ”¹setting
            # logging.info("mid {0} is exist, skip ".format(mid))
            return
        url = response.xpath(self.xpath.get("chapter")).extract_first()
        if not url:
            # todo è¿™é‡Œæ‰è·å–volï¼Œä¼šå¯¼è‡´ä¸Šé¢çš„get_sql_itemé‡Œé¢çš„all_chapters_pagesä¸ºç©ºæ•°ç»„å¯¼è‡´listä¸‹æ ‡0è¶Šç•Œ
            url = response.xpath(self.xpath.get("vol")).extract_first()
        assert url, response.url +" is fuck" 
        first_chapter_url = response.urljoin(url)
        yield scrapy.Request(url=first_chapter_url, callback=self.parse_image_base_url, meta={"item": item, "next_url": response.meta.get("next_url")})

    def parse_image_base_url(self, response):
        url = response.xpath(self.xpath.get("image_base_url")).extract_first()
        # logging.info(response.url + ", " + str(url))
        item = response.meta.get("item")
        # logging.info(item)
        assert item != None
        mid = item.get("mid")
        image_base_url = url[:url.find("/"+str(mid)+"/")]
        item["image_base_url"] = image_base_url
        # self.log(image_base_url)
        self.write_database(item)
        next_url = response.meta.get("next_url")
        logging.info("next url: " + next_url)
        yield scrapy.Request(url=next_url, callback=self.parse_index)

        # è§£æä¸€ä¸ªå®Œæˆäº†ä¹‹åï¼ŒåŠ å…¥next_page_urlï¼Œå¯ä»¥è¿™æ ·æ¯ä¸€ä¸ªitemè§£æå®Œéƒ½ä¼šè¯·æ±‚è¿™ä¸ªï¼Œè™½ç„¶scrapyè‡ªå·±å»é‡äº†ï¼Œä½†æ˜¯æ¯•ç«Ÿå¤šä¸€æ­¥åˆ¤æ–­ï¼Œå…ˆè¿™æ ·å†™å§

    def write_database(self, item):
        # è¿™ä¸ªå†™æ³•ç¡®å®åŠï¼Œä½†æ˜¯è¦æ³¨æ„.values()2/3è¡¨ç°å¥½åƒä¸ä¸€æ ·ï¼Œ3ä¼šæœ‰dictvalueä¹‹ç±»çš„å­—ç¬¦ä¸²ï¼Œæ‰€ä»¥å’Œkeysä¸€æ ·ç”¨joinè¿æ¥å§ï¼Œä½†æ˜¯ã€‚ã€‚ã€‚intå°±è·ªäº†æ¡è‰ï¼Œè¿™æ€ä¹ˆæ•´ï¼Œè½¬tunpleå°±å¥½äº†
        sql = 'insert into {0} ({1}) values ({2})'.format(self.sqlite_table, ', '.join(item.keys()), ', '.join(['?'] * len(item.keys())))
        logging.info("insert mid " + str(item.get("mid")) + ": " + item.get("name") + " category: " + str(item.get("category")))
        values = tuple(item.values())
        # self.log(values)
        self.cur.execute(sql, values)
        self.conn.commit()

    def is_mid_exist(self, mid):
        sql = "select mid from {0} where mid = ? ".format(self.sqlite_table)
        # é€—å·æ˜¯å¿…é¡»çš„ï¼Œä¸ç„¶ä¼šè¢«è§£ææˆæ‹¬å·ï¼Œè€Œä¸æ˜¯tunple
        cursor = self.cur.execute(sql, (mid, ))
        return cursor.fetchone() != None
        # æŸ¥è¯¢åº”è¯¥ä¸ç”¨commitï¼Œsaveçš„æ“ä½œæ‰éœ€è¦
        # self.conn.commit()

    def closed(self, reason):
        if self.conn:
            self.conn.close()















