import scrapy
import sys
import logging
import re
from manga.items import MangaItem
from manga.items import SqliteItem
import sqlite3

class SoulMangaSpider(scrapy.Spider):
    name = "soul_manga"
    xpath = {
        "index_url": [
            "http://www.cartoonmad.com/comic01.html",
            "http://www.cartoonmad.com/comic02.html",
            "http://www.cartoonmad.com/comic03.html",
            "http://www.cartoonmad.com/comic04.html",
            "http://www.cartoonmad.com/comic07.html",
            "http://www.cartoonmad.com/comic08.html",
            "http://www.cartoonmad.com/comic09.html",
            "http://www.cartoonmad.com/comic10.html",
            "http://www.cartoonmad.com/comic13.html",
            "http://www.cartoonmad.com/comic14.html",
            "http://www.cartoonmad.com/comic16.html",
            "http://www.cartoonmad.com/comic17.html",
            "http://www.cartoonmad.com/comic18.html",
            "http://www.cartoonmad.com/comic21.html",
            "http://www.cartoonmad.com/comic22.html",
        ],  
        "urls": ["http://www.cartoonmad.com/comic/1090.html"],
        "chapter": "//a[contains(., 'è©±')]/@href",  # é»˜è®¤ä¸‹è½½è¯
        "vol": "//a[contains(., 'å·')]/@href",  # é»˜è®¤ä¸‹è½½è¯
        "image_page": "//option[contains(., 'é ')]/@value", # éå†è¿™ä¸€è¯çš„æ‰€æœ‰imgé¡µçš„è¶…é“¾æ¥  
        "image": "//img[contains(@src, 'cartoonmad.com')]/@src", #è¿™ä¸€è¯çš„å›¾ç‰‡

        # ä¸çŸ¥é“ä¸ºå•¥ï¼Œchromeé‡Œé¢å¯ä»¥çš„ï¼Œscrapyä¸€ç¢°åˆ°tbodyå°±è·ªã€‚ã€‚åŸå› å¦‚ä¸‹ã€‚ã€‚åŸæ¥tbodyæ˜¯æµè§ˆå™¨åŠ çš„...é‚£å°±ç®€å•äº†ï¼Œæˆ‘å»äº†å°±è¡Œäº†ã€‚ã€‚
        # Firefox, in particular, is known for adding <tbody> elements to tables. Scrapy, on the other hand, does not modify the original page HTML, 
        # so you wonâ€™t be able to extract any data if you use <tbody> in your XPath expressions.

        "mid":"/html/body/table/tr[1]/td[2]/table/tr[3]/td[2]/a[3]/@href",
        "name":"/html/body/table/tr[1]/td[2]/table/tr[3]/td[2]/a[3]/text()",
        "author":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[5]/td/text()",
        "cover_image":"//div[@class='cover']/../img/@src",
        "cover_update_info":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[7]/td/font/text()",
        "category":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[3]/td/a[1]/text()",
        "summary":"//legend[contains(., 'ç°¡ä»‹')]/../table/tr/td/text()",

        "last_update_date":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[1]/td[2]/b/font/text()",
        "status":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[7]/td/img[2]/@src", #chap9.gif
        "pop":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[11]/td/text()",
        "tags":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[13]/td/text()",
        "chapters":"/html/body/table/tr[1]/td[2]/table/tr[4]/td/table/tr[2]/td[2]/table[1]/tr[7]/td/font/text()", 
        "chapter_images":"",
        "vol_or_ch":"", #é€šè¿‡chapter/volè®¾ç½®ï¼Œä¼˜å…ˆè¯

        "all_chapters": "//a[contains(., 'è©±')]/../a/text()",
        "all_chapters_pages": "//a[contains(., 'è©±')]/../font/text()",
        "all_vols": "//a[contains(., 'å·')]/../a/text()",
        "all_vols_pages": "//a[contains(., 'å·')]/../font/text()",
        "image_base_url": "/html/body/table/tr[5]/td/a/img/@src"
    }
    sql_item = {}

    def get_sql_item(self, response):
        # å¼‚æ­¥ä»£ç ï¼Œä¸èƒ½é€šè¿‡selfè·å–ï¼Œè¦ç›´æ¥ä¼ é€’ä¸‹å»ï¼Œé€šè¿‡meta
        sql_item = {}
        sql_item["mid"] = response.xpath(self.xpath.get("mid")).extract_first()
        sql_item["name"] = response.xpath(self.xpath.get("name")).extract_first()
        sql_item["author"] = response.xpath(self.xpath.get("author")).extract_first()
        sql_item["cover_image"] = response.xpath(self.xpath.get("cover_image")).extract_first()
        sql_item["cover_update_info"] = response.xpath(self.xpath.get("cover_update_info")).extract_first()
        sql_item["category"] = response.xpath(self.xpath.get("category")).extract_first()
        sql_item["summary"] = response.xpath(self.xpath.get("summary")).extract()[0]
        sql_item["last_update_date"] = response.xpath(self.xpath.get("last_update_date")).extract()[1]
        sql_item["status"] = response.xpath(self.xpath.get("status")).extract_first()
        sql_item["pop"] = response.xpath(self.xpath.get("pop")).extract_first()
        sql_item["tags"] = response.xpath(self.xpath.get("tags")).extract_first()

        chapters_len = len(response.xpath(self.xpath.get("all_chapters")).extract())
        if(chapters_len != 0):
            sql_item["all_chapters_len"] = chapters_len
            sql_item["all_chapters_pages"] = response.xpath(self.xpath.get("all_chapters_pages")).extract()
            sql_item["vol_or_ch"] = 0
        else:
            sql_item["all_chapters_len"] = len(response.xpath(self.xpath.get("all_vols")).extract())
            sql_item["all_chapters_pages"] = response.xpath(self.xpath.get("all_vols_pages")).extract()
            sql_item["vol_or_ch"] = 1

        # print(sql_item.get("summary"))
        for k, v in sql_item.items():
            if k == 'last_update_date':
                sql_item[k] = str.strip(v)
            elif k == "mid":
                sql_item[k] = int(v[v.rfind("/")+1:v.rfind(".")])
                # logging.info("parse sql item >>>>>>>>>>>>>>>>>> " + str(sql_item.get("mid")))
            elif k == "all_chapters_pages":
                temp = [re.findall(r"\d+", x)[0] for x in v]
                sql_item[k] = ','.join(temp)
            elif k == "all_chapters":
                pass
            elif k in ["name", "author", "cover_update_info", "pop", "summary", "tags"] :
                sql_item[k] = re.sub(r"\s+", "", v, flags=re.UNICODE)
            elif k == "category":
                sql_item[k] = self.get_category(v) # remove ç³»åˆ—
        # print(sql_item)
        return sql_item

    def get_category(self, ori):
        category_map = [
            'æ ¼é¬¥',
            'é­”æ³•',
            'åµæ¢',
            'ç«¶æŠ€',
            'ææ€–',
            'æˆ°åœ‹',
            'é­”å¹»',
            'å†’éšª',
            'æ ¡åœ’',
            'æç¬‘',
            'å°‘å¥³',
            'å°‘ç”·',
            'ç§‘å¹»',
            'æ¸¯ç”¢',
            'å…¶ä»–' 
        ]
        cat = ori[:2]
        assert (cat in category_map)
        return category_map.index(cat)

    def start_requests(self):
        self.sqlite_file = self.settings.get("SQLITE_FILE")
        self.sqlite_table = self.settings.get("SQLITE_TABLE")
        # self.log("fuck " + self.sqlite_file + ", " + self.sqlite_table)
        self.conn = sqlite3.connect(self.sqlite_file)
        self.cur = self.conn.cursor()

        # è·å–å…¨é¡µæ¼«ç”»
        urls = self.xpath.get("index_url")
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse_all)

        # è·å–å•ä¸ªæ¼«ç”»
        # urls = self.xpath.get("urls")
        # for url in urls:
        #     yield scrapy.Request(url=url, callback=self.parse)
            # è¿™ä¸€æ­¥å®Œæˆä¹‹åå°±æŠŠæ‰€æœ‰çš„åŸºæœ¬ä¿¡æ¯å–åˆ°ï¼Œæˆ‘èƒ½åŒæ—¶è°ƒç”¨å—ï¼Ÿå¥½åƒä¸è¡Œï¼Œyieldæœ‰returnè¯­ä¹‰çš„ï¼Œä¸èƒ½è¿”ä¸¤ä¸ªreturnå§ã€‚ã€‚é‚£å°±èµ°parseå§ï¼Œç„¶åæ ‡è®°çŠ¶æ€ï¼Œå®Œæˆä¸€æ¬¡ä¹‹åå°±ä¸å†å†™å…¥åŸºæœ¬ä¿¡æ¯äº†


    def parse_all(self, response):
        mangas = re.findall(r"comic/\d{4}.html", str(response.body))[:15]
        if response.url.find("/comic/") != -1:
            mangas = [x[6:] for x in mangas]
        # self.log(mangas)
        # é›†åˆæ¨å¯¼ä½¿ç”¨{}
        urls = {response.urljoin(x) for x in mangas}
        self.log(urls)

        # # è¿™æ ·å°±æŠŠå½“å‰é¡µ(index_url)åŒ…å«çš„æ‰€æœ‰æ¼«ç”»éƒ½çˆ¬äº†ğŸ˜¯
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)


    def parse(self, response):
        # å…¶å®è¿™é‡Œæœ¬æ¥æ¯ä¸ªæ¼«ç”»çš„urlä¹Ÿå°±èµ°ä¸€æ¬¡å§ã€‚ã€‚ã€‚ç®€ç›´å®Œç¾
        item = self.get_sql_item(response)
        url = response.xpath(self.xpath.get("chapter")).extract_first()
        if not url:
            url = response.xpath(self.xpath.get("vol")).extract_first()
        assert url 
        first_chapter_url = response.urljoin(url)
        # self.log("fuck " + first_chapter_url)
        yield scrapy.Request(url=first_chapter_url, callback=self.parse_image_base_url, meta={"item": item})

    def parse_image_base_url(self, response):
        url = response.xpath(self.xpath.get("image_base_url")).extract_first()
        item = response.meta.get("item")
        assert item != None
        mid = item.get("mid")
        image_base_url = url[:url.find("/"+str(mid)+"/")]
        item["image_base_url"] = image_base_url
        # self.log(image_base_url)
        self.write_database(item)

    def write_database(self, item):
        # è¿™ä¸ªå†™æ³•ç¡®å®åŠï¼Œä½†æ˜¯è¦æ³¨æ„.values()2/3è¡¨ç°å¥½åƒä¸ä¸€æ ·ï¼Œ3ä¼šæœ‰dictvalueä¹‹ç±»çš„å­—ç¬¦ä¸²ï¼Œæ‰€ä»¥å’Œkeysä¸€æ ·ç”¨joinè¿æ¥å§ï¼Œä½†æ˜¯ã€‚ã€‚ã€‚intå°±è·ªäº†æ¡è‰ï¼Œè¿™æ€ä¹ˆæ•´ï¼Œè½¬tunpleå°±å¥½äº†
        sql = 'insert into {0} ({1}) values ({2})'.format(self.sqlite_table, ', '.join(item.keys()), ', '.join(['?'] * len(item.keys())))
        logging.info("insert mid " + str(item.get("mid")) + ": " + item.get("name") + " category: " + str(item.get("category")))
        values = tuple(item.values())
        # self.log(values)
        self.cur.execute(sql, values)
        self.conn.commit()

    def closed(self, reason):
        if self.conn:
            self.conn.close()















